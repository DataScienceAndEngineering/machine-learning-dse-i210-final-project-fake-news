# -*- coding: utf-8 -*-
"""Final_Model_Building.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xdR8RGGeTl94fvNz-YLePUbXrgjsZvxe

# **Data Load**
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import drive
# Mount your Google Drive
drive.mount('/content/drive')

# Read the dataset from the specified path
df = pd.read_csv('/content/drive/MyDrive/ML project/spaCy_Word2Vec.csv', sep=',', encoding='utf-8', quotechar='"')
df.head()

unique_types = df['vector'].apply(type).unique()
print(unique_types)

"""# Models"""

from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report
import numpy as np

# Padding or truncating function to ensure all vectors have the same length
def pad_or_truncate_vector(vector, length):
    if isinstance(vector, np.ndarray):
        if len(vector) < length:
            # Pad vector with zeros if it's shorter than the specified length
            return np.pad(vector, (0, length - len(vector)))
        elif len(vector) > length:
            # Truncate vector if it's longer than the specified length
            return vector[:length]
        else:
            return vector
    else:
        # Return a zero-filled vector if the input is not a numpy array
        return np.zeros(length)

# Define the desired length for the vectors
max_length = 100  # Adjust this value according to your data

# Pad or truncate vectors to the desired length
df['vector'] = df['vector'].apply(pad_or_truncate_vector, length=max_length)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['vector'], df['type'], test_size=0.2, random_state=42)

# List of classifiers to compare
classifiers = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(),
    "Decision Tree": DecisionTreeClassifier(),
    "Support Vector Machine": SVC(),
    "Gradient Boosting": GradientBoostingClassifier()
}

# Set up cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Hyperparameter grids
param_grids = {
    "Logistic Regression": {'C': [0.1, 1, 10]},
    "Random Forest": {'n_estimators': [100, 200]},
    "Decision Tree": {'max_depth': [None, 10, 20]},
    "Support Vector Machine": {'C': [1, 10], 'gamma': [0.01, 0.1]},
    "Gradient Boosting": {'n_estimators': [100, 200], 'learning_rate': [0.1, 1.0]}
}

# Iterate over each classifier
for name, classifier in classifiers.items():
    print(f"--- {name} ---")

    # Hyperparameter tuning using GridSearchCV
    param_grid = param_grids[name]
    grid_search = GridSearchCV(classifier, param_grid, cv=cv, scoring='accuracy')
    grid_search.fit(list(X_train), y_train)

    # Best hyperparameters
    print("Best hyperparameters:", grid_search.best_params_)

    # Train the model on the full training data with best hyperparameters
    best_classifier = grid_search.best_estimator_
    best_classifier.fit(list(X_train), y_train)

    # Predict on the testing data
    y_pred = best_classifier.predict(list(X_test))

    # Evaluation
    print(classification_report(y_test, y_pred))

import warnings

# Iterate over each classifier
for name, classifier in classifiers.items():
    print(f"--- {name} ---")

    # Train the model on the training data
    classifier.fit(list(X_train), y_train)

    # Predict on the testing data
    y_pred = classifier.predict(list(X_test))

    # Print the classification report without warnings
    with np.errstate(divide='ignore'):
        print(classification_report(y_test, y_pred, zero_division=1))

import matplotlib.pyplot as plt
# Accuracy scores of each model
accuracy_scores = {
    "Logistic Regression": 0.95,
    "Random Forest": 0.96,
    "Decision Tree": 0.90,
    "Support Vector Machine": 0.96,
    "Gradient Boosting": 0.94
}

# Plotting the bar graph
plt.figure(figsize=(10, 6))
plt.bar(accuracy_scores.keys(), accuracy_scores.values(), color='skyblue')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.title('Accuracy of Different Models')
plt.ylim(0.85, 1.0)  # Set y-axis limit for better visualization
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.show()

# Iterate over each classifier
for name, classifier in classifiers.items():
    print(f"--- {name} ---")

    # Train the model on the training data
    classifier.fit(list(X_train), y_train)

    # Predict on the testing data
    y_pred = classifier.predict(list(X_test))

    # Calculate confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # Print confusion matrix
    print("Confusion Matrix:")
    print(cm)
    print()

# Calculate number of classifiers
num_classifiers = len(classifiers)

# Calculate number of rows and columns for subplots
num_rows = (num_classifiers + 1) // 2  # Add 1 to ensure we have enough rows for odd numbers
num_cols = 2

# Create subplots
fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, num_rows * 6))

# Flatten axes if only one row
if num_rows == 1:
    axes = [axes]

# Iterate over each classifier
for (name, classifier), ax in zip(classifiers.items(), axes.flatten()):
    # Train the model on the training data
    classifier.fit(list(X_train), y_train)

    # Predict on the testing data
    y_pred = classifier.predict(list(X_test))

    # Calculate confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # Plot confusion matrix
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Fake', 'True'], yticklabels=['Fake', 'True'], ax=ax)
    ax.set_title(f'Confusion Matrix - {name}')
    ax.set_xlabel('Predicted')
    ax.set_ylabel('Actual')

# Adjust layout
plt.tight_layout()
plt.show()

# Iterate over each classifier
for name, classifier in classifiers.items():
    print(f"--- {name} ---")

    # Train the model on the training data
    classifier.fit(list(X_train), y_train)

    # Predict on the testing data
    y_pred = classifier.predict(list(X_test))

    # Calculate confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # Print confusion matrix
    print("Confusion Matrix:")
    print(cm)
    print()

# Calculate number of classifiers
num_classifiers = len(classifiers)

# Calculate number of rows and columns for subplots
num_rows = (num_classifiers + 1) // 2  # Add 1 to ensure we have enough rows for odd numbers
num_cols = 2

# Create subplots
fig, axes = plt.subplots(num_rows, num_cols, figsize=(6, num_rows * 6))

# Flatten axes if only one row
if num_rows == 1:
    axes = [axes]

# Iterate over each classifier
for (name, classifier), ax in zip(classifiers.items(), axes.flatten()):
    # Train the model on the training data
    classifier.fit(list(X_train), y_train)

    # Predict on the testing data
    y_pred = classifier.predict(list(X_test))

    # Calculate confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # Plot confusion matrix
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Fake', 'True'], yticklabels=['Fake', 'True'], ax=ax)
    ax.set_title(f'Confusion Matrix - {name}')
    ax.set_xlabel('Predicted')
    ax.set_ylabel('Actual')

# Adjust layout
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Define the data
classifiers = ['Logistic Regression', 'Random Forest', 'Decision Tree', 'Support Vector Machine', 'Gradient Boosting']
true_positives = [4387, 4478, 4304, 4445, 4326]
false_positives = [263, 172, 346, 205, 324]
true_negatives = [4123, 4162, 3839, 4196, 4074]
false_negatives = [207, 168, 491, 134, 256]

# Create subplots
fig, ax = plt.subplots(figsize=(12, 8))

# Plot bars for each classifier
bar_width = 0.2
index = range(len(classifiers))
opacity = 0.8

plt.bar(index, true_positives, bar_width, alpha=opacity, color='b', label='True Positives')
plt.bar([i + bar_width for i in index], false_positives, bar_width, alpha=opacity, color='r', label='False Positives')
plt.bar([i + 2*bar_width for i in index], true_negatives, bar_width, alpha=opacity, color='g', label='True Negatives')
plt.bar([i + 3*bar_width for i in index], false_negatives, bar_width, alpha=opacity, color='y', label='False Negatives')

plt.xlabel('Classifiers')
plt.ylabel('Counts')
plt.title('Confusion Matrix Counts by Classifier')
plt.xticks([i + 1.5*bar_width for i in index], classifiers)
plt.legend()

plt.tight_layout()
plt.show()