{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f768f51ff419d",
   "metadata": {},
   "source": [
    "**Abhina Premachandran Bindu**\n",
    "\n",
    "**April 30 2024**\n",
    "# Comparing the performance of gensim vs nltk libraries\n",
    "<p> In this analysis, the nltk and gensim nlp libraries are compared based on the accuracy scores of the same classifier applied on the processed texts corresponding to the libraries. It is found that using nltk library to process the text and tfidf vectorizer to apply the classifier resulted in better accuracy scores compared to using gensim's word2vect function for training the classifier. The accuracy for nltk is 0.99 for the gradient boosting classifier while the gensim accuracy for the same classifier is only 0.96.</p>\n",
    "\n",
    "## Loading and initial cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T17:59:33.588662Z",
     "start_time": "2024-04-30T17:59:32.019411Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing the necessary libraries\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# importing the libraries for nltk\n",
    "import nltk\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# BoW\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a410d05af240b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T16:58:11.051998Z",
     "start_time": "2024-04-30T16:57:52.225495Z"
    }
   },
   "outputs": [],
   "source": [
    "#importing the data\n",
    "data = pd.read_csv(input('Enter the file path for the csv file:'))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f953265c8e649",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T16:58:12.759444Z",
     "start_time": "2024-04-30T16:58:12.751967Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa469c0361198",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T16:58:13.012259Z",
     "start_time": "2024-04-30T16:58:12.990119Z"
    }
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c5d09acbbb714",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T16:58:13.210467Z",
     "start_time": "2024-04-30T16:58:13.198866Z"
    }
   },
   "outputs": [],
   "source": [
    "# dropping the na values\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770d0576792227a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T16:58:13.778355Z",
     "start_time": "2024-04-30T16:58:13.769028Z"
    }
   },
   "outputs": [],
   "source": [
    "# dropping the redundant 'Unnamed: 0' column\n",
    "data.drop(columns='Unnamed: 0',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380d38d8d7566387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T16:58:14.140195Z",
     "start_time": "2024-04-30T16:58:14.133683Z"
    }
   },
   "outputs": [],
   "source": [
    "# checking the value counts of 'target' to check for data imbalance\n",
    "data.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc56e418debe90",
   "metadata": {},
   "source": [
    " Since the number of Fake and True classes are almost same, there is no class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766bca5964e15350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T16:58:15.020468Z",
     "start_time": "2024-04-30T16:58:15.014725Z"
    }
   },
   "outputs": [],
   "source": [
    "data.subject.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b0b5df254691f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22994fa5cd64e4e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T16:58:16.602215Z",
     "start_time": "2024-04-30T16:58:16.573917Z"
    }
   },
   "outputs": [],
   "source": [
    "# encoding the class labels to numerical - Real:1 and Fake:0\n",
    "class_mapping = {label: idx for idx, label in enumerate(np.unique(data['target']))}\n",
    "data['target'] = data['target'].map(class_mapping)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a981875",
   "metadata": {},
   "source": [
    "## using nltk for cleaning and preparing for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67078c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and removing stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # remove non-alphabetical characters and stopwords\n",
    "    cleaned_tokens = [re.sub(r'[^a-zA-Z ]', '', text).lower() for text in tokens if text.lower() not in stop_words]\n",
    "    cleaned_tokens = [token for token in cleaned_tokens if ((token not in  set(string.punctuation)))]\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\n",
    "    # Join the tokens back into a string\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    #stem the tokens\n",
    "    porter = PorterStemmer()\n",
    "    cleaned_text = \" \".join(porter.stem(token) for token in processed_text.split())\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the function across the DataFrame\n",
    "data_nltk = data.copy()\n",
    "data_nltk['cleaned_text'] = data_nltk['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b734a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nltk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8589b011",
   "metadata": {},
   "source": [
    "## using gensim to clean and build the vectors for the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b35f4ab0e8b04c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T16:58:28.076405Z",
     "start_time": "2024-04-30T16:58:17.473031Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the function across the DataFrame\n",
    "data_gensim = data.copy()\n",
    "data_gensim['cleaned_text'] = data_gensim['text'].apply(gensim.utils.simple_preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f613c87041eb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T16:58:28.095236Z",
     "start_time": "2024-04-30T16:58:28.079Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1529ac2",
   "metadata": {},
   "source": [
    "## Building, training and using the gensim word2vect model for getting the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b672d749b5e307",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T16:59:56.831807Z",
     "start_time": "2024-04-30T16:59:53.366397Z"
    }
   },
   "outputs": [],
   "source": [
    "# building the word2vec model\n",
    "model = gensim.models.Word2Vec(\n",
    "    window = 6,\n",
    "    min_count = 1,\n",
    "    workers = 4\n",
    ")\n",
    "model.build_vocab(data_gensim['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77f44e4c18b3166",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T17:00:30.713369Z",
     "start_time": "2024-04-30T16:59:58.692681Z"
    }
   },
   "outputs": [],
   "source": [
    "# training the model\n",
    "model.train(data_gensim['cleaned_text'], total_examples=model.corpus_count, epochs=5)\n",
    "\n",
    "# saving the model\n",
    "model.save(\"word2vec/word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f46ff7246eda5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T17:00:32.652147Z",
     "start_time": "2024-04-30T17:00:32.648113Z"
    }
   },
   "outputs": [],
   "source": [
    "model.wv.index_to_key[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd66cead82b8c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T17:00:34.545777Z",
     "start_time": "2024-04-30T17:00:34.541757Z"
    }
   },
   "outputs": [],
   "source": [
    "len(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbebf91701154dc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T17:00:52.799753Z",
     "start_time": "2024-04-30T17:00:36.271109Z"
    }
   },
   "outputs": [],
   "source": [
    "# a function for finding the average of the word vectors \n",
    "def get_average_word2vec_vector(text, model, word_dim):\n",
    "  vec = np.zeros((word_dim,))  \n",
    "  count = 0\n",
    "  for word in text:\n",
    "    if word in model.wv:  \n",
    "      vec += model.wv[word]\n",
    "      count += 1\n",
    "  if count != 0:\n",
    "    vec /= count  \n",
    "  return vec\n",
    "\n",
    "# Get word dimensions from the model\n",
    "word_dim = model.vector_size\n",
    "\n",
    "# Apply the function to each cleaned_text\n",
    "word_vectors = [get_average_word2vec_vector(text, model, word_dim) for text in data_gensim['cleaned_text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e3c485bc59c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T17:00:52.810935Z",
     "start_time": "2024-04-30T17:00:52.802352Z"
    }
   },
   "outputs": [],
   "source": [
    "# adding the word vectors to the data\n",
    "data['word_vectors'] = word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08cc3fca3edfe39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T17:00:58.306107Z",
     "start_time": "2024-04-30T17:00:58.294614Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d6fd4c",
   "metadata": {},
   "source": [
    "## Classifying the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee36f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the model\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "# importing necessary libraries for model building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "# defining the model\n",
    "clf = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492400931c593c5",
   "metadata": {},
   "source": [
    "### using nltk and tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0127fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the X and y arrays for training and testing\n",
    "X1 = data_nltk['cleaned_text'].values\n",
    "y1 = data_nltk['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31470a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.shape,y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea8bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data to train-test split\n",
    "X_train1,X_test1,y_train1,y_test1 = train_test_split(X1,y1,test_size=0.33,random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b19a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the tfidf vectorizer\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b15a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the pipeline to fit the training data\n",
    "gb_tfidf = Pipeline([\n",
    "    ('vect',tfidf),\n",
    "    ('gb clf',clf)\n",
    "])\n",
    "gb_tfidf.fit(X_train1,y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42371e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = gb_tfidf.predict(X_test1)\n",
    "# printing the classification report for validation of the model\n",
    "print(classification_report(y_test1, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac2b0cf",
   "metadata": {},
   "source": [
    "### using gensim - word2vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d63d09a35b8e1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T17:01:04.711155Z",
     "start_time": "2024-04-30T17:01:04.708716Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining X and y arrays for training and testing\n",
    "X2 = word_vectors\n",
    "y2 = data['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556f22920bc2336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T17:01:05.135937Z",
     "start_time": "2024-04-30T17:01:05.121791Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.33, random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587172b0dedee0a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T17:01:07.358359Z",
     "start_time": "2024-04-30T17:01:07.300929Z"
    }
   },
   "outputs": [],
   "source": [
    "# reshaping the input values for classifying\n",
    "X_train_2d = np.stack(X_train2)\n",
    "X_test_2d =  np.stack(X_test2)\n",
    "X_train_2d.shape , X_test_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bad347a4fe25e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T17:03:19.572238Z",
     "start_time": "2024-04-30T17:01:07.651719Z"
    }
   },
   "outputs": [],
   "source": [
    "# fitting the train data\n",
    "clf.fit(X_train_2d, y_train2)\n",
    "# predicting the test values\n",
    "y_pred2 = clf.predict(X_test_2d)\n",
    "# printing the classification report for validation of the model\n",
    "print(classification_report(y_test2, y_pred2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
