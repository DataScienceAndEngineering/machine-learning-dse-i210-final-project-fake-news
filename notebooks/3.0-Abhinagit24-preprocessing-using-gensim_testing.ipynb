{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f768f51ff419d",
   "metadata": {},
   "source": [
    "# using Gensim Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T20:04:05.877507Z",
     "start_time": "2024-04-22T20:03:33.086392Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a410d05af240b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:00:06.456331Z",
     "start_time": "2024-04-22T17:00:03.169731Z"
    }
   },
   "outputs": [],
   "source": [
    "# combining the two separate csv files with fake and real data to a single dataframe \n",
    "# df1 --> Fake , df2 --> Real\n",
    "df1 = pd.read_csv(input(\"Enter the file path for the fake dataset\"))\n",
    "df2 = pd.read_csv(input(\"Enter the file path for the real dataset\"))\n",
    "\n",
    "# adding the labels Fake --> 0 and Real --> 1\n",
    "df1['target'] = 0\n",
    "df2['target'] = 1\n",
    "\n",
    "# combining the dataframes\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "# shuffling the indices\n",
    "data = combined_df.sample(frac=1, random_state=42)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f953265c8e649",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:00:07.399726Z",
     "start_time": "2024-04-22T17:00:07.392684Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa469c0361198",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:00:07.977642Z",
     "start_time": "2024-04-22T17:00:07.962025Z"
    }
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c5d09acbbb714",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:00:08.387678Z",
     "start_time": "2024-04-22T17:00:08.376109Z"
    }
   },
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380d38d8d7566387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:00:09.374938Z",
     "start_time": "2024-04-22T17:00:09.368576Z"
    }
   },
   "outputs": [],
   "source": [
    "data.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc56e418debe90",
   "metadata": {},
   "source": [
    " Since the number of Fake and True classes are almost same, there is no class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766bca5964e15350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:00:10.486439Z",
     "start_time": "2024-04-22T17:00:10.477491Z"
    }
   },
   "outputs": [],
   "source": [
    "data.subject.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f11dd8df7a632",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:00:12.205155Z",
     "start_time": "2024-04-22T17:00:11.999022Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the distribution of subjects\n",
    "plt.hist(data['subject'], bins=len(data.subject.unique()), align = 'mid', edgecolor='black')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Subjects')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b0b5df254691f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b35f4ab0e8b04c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T20:09:37.160620Z",
     "start_time": "2024-04-22T20:09:25.401554Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the function across the DataFrame\n",
    "data['cleaned_text'] = data['text'].apply(lambda x: gensim.utils.simple_preprocess(x,max_len=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f613c87041eb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T20:09:39.813083Z",
     "start_time": "2024-04-22T20:09:39.791772Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b672d749b5e307",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T20:10:06.962290Z",
     "start_time": "2024-04-22T20:10:00.737796Z"
    }
   },
   "outputs": [],
   "source": [
    "# building the word2vec model based on the dataset\n",
    "model = gensim.models.Word2Vec(\n",
    "    window = 6,\n",
    "    min_count = 1,\n",
    "    workers = 4\n",
    ")\n",
    "model.build_vocab(data['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77f44e4c18b3166",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T20:11:00.876251Z",
     "start_time": "2024-04-22T20:10:09.619787Z"
    }
   },
   "outputs": [],
   "source": [
    "# training the model\n",
    "model.train(data['cleaned_text'], total_examples=model.corpus_count, epochs=5)\n",
    "\n",
    "# saving the model\n",
    "model.save(\"word2vec/word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f46ff7246eda5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T20:11:00.885346Z",
     "start_time": "2024-04-22T20:11:00.879905Z"
    }
   },
   "outputs": [],
   "source": [
    "model.wv.index_to_key[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd66cead82b8c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T20:11:00.889312Z",
     "start_time": "2024-04-22T20:11:00.886569Z"
    }
   },
   "outputs": [],
   "source": [
    "len(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbebf91701154dc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T20:14:01.627494Z",
     "start_time": "2024-04-22T20:13:41.844462Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate the average word vector for a sentence\n",
    "def get_average_word2vec_vector(text, model, word_dim):\n",
    "  vec = np.zeros((word_dim,))  \n",
    "  count = 0\n",
    "  for word in text:\n",
    "    if word in model.wv:  \n",
    "      vec += model.wv[word]\n",
    "      count += 1\n",
    "  if count != 0:\n",
    "    vec /= count  \n",
    "  return vec\n",
    "\n",
    "# Get word dimensions from the model\n",
    "word_dim = model.vector_size\n",
    "\n",
    "# Apply the function to each preprocessed news text\n",
    "word_vectors1 = [get_average_word2vec_vector(text, model, word_dim) for text in data['cleaned_text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dfa1fab1c64fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# defining the wordvector for the google \n",
    "def word_vec(sent):\n",
    "    vector_size = wv.vector_size\n",
    "    wv_res = np.zeros(vector_size)\n",
    "    print(wv_res)\n",
    "    ctr = 1\n",
    "    for w in sent:\n",
    "        if w in wv:\n",
    "            ctr += 1\n",
    "            wv_res += wv[w]\n",
    "    wv_res = wv_res/ctr\n",
    "    return wv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e3c485bc59c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:01:31.709888Z",
     "start_time": "2024-04-22T17:01:31.701562Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data['word_vectors2'] = data['cleaned_text'].apply(word_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd5f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492400931c593c5",
   "metadata": {},
   "source": [
    "## Classifying the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78df15071a62378",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:04:30.860678Z",
     "start_time": "2024-04-22T17:04:30.020940Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing necessary libraries for model building\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d63d09a35b8e1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:04:33.656867Z",
     "start_time": "2024-04-22T17:04:33.653901Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining X and y \n",
    "X = word_vectors1\n",
    "X2 = data['word_vectors2'].values\n",
    "y = data['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556f22920bc2336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:06:58.215927Z",
     "start_time": "2024-04-22T17:06:58.190342Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=44)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y, test_size=0.33, random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587172b0dedee0a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:07:00.855224Z",
     "start_time": "2024-04-22T17:07:00.778698Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_2d = np.stack(X_train)\n",
    "X_test_2d =  np.stack(X_test)\n",
    "X_train2_2d = np.stack(X_train2)\n",
    "X_test2_2d =  np.stack(X_test2)\n",
    "X_train_2d.shape , X_test_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bad347a4fe25e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:09:33.240494Z",
     "start_time": "2024-04-22T17:07:19.410702Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# creating a GradientBoosting model\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "# fit with all_train_embeddings and y_train\n",
    "clf.fit(X_train_2d, y_train)\n",
    "\n",
    "\n",
    "# get the predictions for all_test_embeddings and store it in y_pred\n",
    "y_pred = clf.predict(X_test_2d)\n",
    "\n",
    "\n",
    "# print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc176595",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = GradientBoostingClassifier()\n",
    "clf2.fit(X_train2_2d, y_train2)\n",
    "y_pred2 = clf.predict(X_test2_2d)\n",
    "print(classification_report(y_test2, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2208cc51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
