{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f768f51ff419d",
   "metadata": {},
   "source": [
    "**Abhina Premachandran Bindu**\n",
    "\n",
    "**May 6 2024**\n",
    "\n",
    "# Preprocessing the dataset using Gensim Library\n",
    "  <p> The goal of this notebook is to explain the working of the classifier. A Decision Tree classifier is used to fit and train on the word embeddings. To understand the working of the classifier, shap plots are used for individual test data. Further, the feature importance is found using the feature_importances_ attribute of the DecisionTree classifier. </p>\n",
    "  \n",
    "  \n",
    "## Loading and initial cleaning of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:14:12.339618Z",
     "start_time": "2024-04-15T19:13:35.985792Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# importing nlp\n",
    "import nltk\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# importing sklearn for model building\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# importing shap\n",
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1c11d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:14:18.560168Z",
     "start_time": "2024-04-15T19:14:12.350041Z"
    }
   },
   "outputs": [],
   "source": [
    "# combining the two separate csv files with fake and real data to a single dataframe \n",
    "# df1 --> Fake , df2 --> Real\n",
    "df1 = pd.read_csv(input(\"Enter the file path for the fake dataset\"))\n",
    "df2 = pd.read_csv(input(\"Enter the file path for the real dataset\"))\n",
    "\n",
    "# adding the labels Fake --> 0 and Real --> 1\n",
    "df1['target'] = 0\n",
    "df2['target'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a410d05af240b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:14:18.560168Z",
     "start_time": "2024-04-15T19:14:12.350041Z"
    }
   },
   "outputs": [],
   "source": [
    "# combining the dataframes\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "# shuffling the indices\n",
    "data = combined_df.sample(frac=1, random_state=42)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data.to_csv('fake_real_final.csv', index = False)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f953265c8e649",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:14:18.726669Z",
     "start_time": "2024-04-15T19:14:18.561643Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa469c0361198",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:14:18.753125Z",
     "start_time": "2024-04-15T19:14:18.727513Z"
    }
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380d38d8d7566387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:14:18.792637Z",
     "start_time": "2024-04-15T19:14:18.783529Z"
    }
   },
   "outputs": [],
   "source": [
    "# checking the value counts of 'target' to check for data imbalance\n",
    "data.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc56e418debe90",
   "metadata": {},
   "source": [
    " Since the number of Fake and True classes are almost same, there is no class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766bca5964e15350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:14:18.799744Z",
     "start_time": "2024-04-15T19:14:18.794324Z"
    }
   },
   "outputs": [],
   "source": [
    "data.subject.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f11dd8df7a632",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:14:19.098831Z",
     "start_time": "2024-04-15T19:14:18.800613Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the distribution of subjects\n",
    "data_real = data[data['target']==1]\n",
    "data_fake = data[data['target']==0]\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(10, 12))\n",
    "# Plot the most frequent words in real news on the first subplot\n",
    "axs[0].hist(data_real['subject'], bins=len(data_real.subject.unique()), align = 'mid', edgecolor='black', color='blue')\n",
    "axs[0].set_xlabel('Subjects')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "axs[0].set_title('Real News Subjects')\n",
    "axs[0].tick_params(axis='x') \n",
    "axs[0].legend(['Real News'])\n",
    "\n",
    "axs[1].hist(data_fake['subject'], bins=len(data_fake.subject.unique()), align = 'mid', edgecolor='black', color = 'orange')\n",
    "axs[1].set_xlabel('Subjects')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "axs[1].set_title('Fake News Subjects')\n",
    "axs[1].tick_params(axis='x', rotation=45) \n",
    "axs[1].legend(['Fake News'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b0b5df254691f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e1180fd44141a",
   "metadata": {},
   "source": [
    "### cleaning and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b35f4ab0e8b04c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:14:37.379701Z",
     "start_time": "2024-04-15T19:14:19.132821Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize and removing stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # remove non-alphabetical characters and stopwords\n",
    "    cleaned_tokens = [re.sub(r'[^a-zA-Z ]', '', text).lower() for text in tokens if text.lower() not in stop_words]\n",
    "    cleaned_tokens = [token for token in cleaned_tokens if ((token not in  set(string.punctuation)))]\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\n",
    "    # Join the tokens back into a string\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    #stem the tokens\n",
    "    porter = PorterStemmer()\n",
    "    cleaned_text = \" \".join(porter.stem(token) for token in processed_text.split())\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the function across the DataFrame\n",
    "data['cleaned_text'] = data['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f613c87041eb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T19:14:37.501071Z",
     "start_time": "2024-04-15T19:14:37.420737Z"
    }
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5766467e",
   "metadata": {},
   "source": [
    "## Classifying the data using DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6dc81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining X and y arrays\n",
    "# X = word_vectors\n",
    "X = data['cleaned_text'].values\n",
    "y = data['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0db061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b9f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the Tfidf vectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=10)\n",
    "X_train_vec = vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_vec = vectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38386d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the classification model\n",
    "tree_clf = tree.DecisionTreeClassifier()\n",
    "tree_clf.fit(X_train_vec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cd9375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the test values\n",
    "y_pred = tree_clf.predict(X_test_vec)\n",
    "# printing the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0308f99",
   "metadata": {},
   "source": [
    "## Understanding the classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68bf5e0",
   "metadata": {},
   "source": [
    "### 1. Using Shap for Decision Tree clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84197dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the feature names from tfidf vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "# getting the shap values\n",
    "explainer = shap.Explainer(tree_clf, X_train_vec, feature_names=feature_names)\n",
    "shap_values = explainer(X_test_vec)\n",
    "print(shap_values.values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ce7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the shap waterfall plot for the 7th test data\n",
    "shap.initjs()\n",
    "\n",
    "ind = 6\n",
    "print(X_test[ind])\n",
    "\n",
    "shap.plots.waterfall(shap_values[ind,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4585be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the shap waterfall plot for the 11th test data\n",
    "shap.initjs()\n",
    "\n",
    "ind = 10\n",
    "print(X_test[ind])\n",
    "\n",
    "shap.plots.waterfall(shap_values[ind,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a65a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the shap waterfall plot for the 201st test data\n",
    "shap.initjs()\n",
    "\n",
    "ind = 1\n",
    "print(X_test[ind])\n",
    "\n",
    "shap.plots.waterfall(shap_values[ind,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0f306f",
   "metadata": {},
   "source": [
    "  From the three waterfall plots above, it is clear that the model uses the word 'reuter' as the primary indicator of whether a text classifies as fake or real. If the shap value of 'reuter' is greater than 0, it classifies the text as real and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048930ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:,feature_names.tolist().index(\"reuter\"),1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d569abc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:,feature_names.tolist().index(\"reuter\"),0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70242200",
   "metadata": {},
   "source": [
    "The above plot shows how the 'reuter' feature influences the model in predicting an object as class 1 - real. Most of the shap values for this feature lies closer to 0.5 implying its importance for the classifier in predicting classes as real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf9dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(\n",
    "    shap_values[:,:,1], X_test_vec, feature_names=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9584ba",
   "metadata": {},
   "source": [
    "### 2. Using Decision Tree classifier features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330990e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_names = [\"Fake\", \"Real\"]\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "vis = tree.plot_tree(\n",
    "    tree_clf,\n",
    "    class_names=class_names,\n",
    "    feature_names = vectorizer.get_feature_names_out(),\n",
    "    max_depth=3,\n",
    "    fontsize=9,\n",
    "    proportion=True,\n",
    "    filled=True,\n",
    "    rounded=True\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4bb58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_importance = tree_clf.feature_importances_\n",
    "inds = np.argsort(np.abs(feature_importance))[::-1]\n",
    "top_10_inds = inds[:10]\n",
    "fig, ax = plt.subplots()\n",
    "rank = np.arange(10)\n",
    "ax.bar(rank, feature_importance[top_10_inds])\n",
    "ax.set_xticks(rank)\n",
    "ax.set_xticklabels(np.array(feature_names)[top_10_inds], rotation=45, ha='right')\n",
    "ax.set_ylabel(\"Top 10 Important Features and their ranks\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51538834",
   "metadata": {},
   "source": [
    "  The above tree visualization of the classifier indicates that the classifier uses 'reuter' feature as one of the main feature to decide whether the text is fake or real. In the next level, 'zika' and 'wiretv' are used to split the data into the respective classes based on certain threshold values for the features. The bar chart on the feature importance also indicates that the 'reuter' feature have a huge significance in influencing the model decision compared to other features. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
