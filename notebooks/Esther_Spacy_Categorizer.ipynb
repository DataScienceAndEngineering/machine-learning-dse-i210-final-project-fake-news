{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification with Spacy Categorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are many different models available for binary classification tasks, spaCy, a natural language processing module also offers it's own trainable model, specifically tailored to text / language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This spaCy function is designed for multi-lablel text classification tasks. It's dependent on training the spaCy model on provided data and produce classification labels on provided text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import dropbox\n",
    "import io\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esthe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\esthe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# load the english model\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe loaded from Dropbox:\n",
      "   Unnamed: 0                                            Content  \\\n",
      "0           0  new york reuters us environmental group sierra...   \n",
      "1           1  washington reuters us air force asked industry...   \n",
      "2           2  saturday paul ryan posted photo instagram phot...   \n",
      "3           3  america keeps waiting word hillary indicted ob...   \n",
      "4           4                   religion peace ht weasel zippers   \n",
      "\n",
      "          Title  Type  \n",
      "0  politicsNews  true  \n",
      "1  politicsNews  true  \n",
      "2          News  fake  \n",
      "3      politics  fake  \n",
      "4     left-news  fake  \n"
     ]
    }
   ],
   "source": [
    "dropbox_access_token = 'sl.B0dlxXsEpFckNW5wQQm_18DoQZEJ0nh17I-TGUvUuoU5vaeLaMY5a8fKIwyH1cyYQfGn3GX06YMuJN1Zzl1Ja7b-UXxQH-I8OLzJylOIX86bQSmXtH4PC7jqICOdKKt5FTTIzSq9Ndor'\n",
    "\n",
    "# Initialize Dropbox client\n",
    "dbx = dropbox.Dropbox(dropbox_access_token)\n",
    "\n",
    "# Dropbox file path\n",
    "dropbox_file_path = '/fake_real_ML_project_dataset.csv'\n",
    "\n",
    "# Download the file\n",
    "metadata, response = dbx.files_download(dropbox_file_path)\n",
    "\n",
    "# Read the CSV file from the response content using io.BytesIO\n",
    "df = pd.read_csv(io.BytesIO(response.content))\n",
    "\n",
    "# Show the results\n",
    "print(\"Dataframe loaded from Dropbox:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove extra column\n",
    "df = df.drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the DataFrame into two subsets based on categories\n",
    "true_df = df[df['Type'] == 'true']\n",
    "fake_df = df[df['Type'] == 'fake']\n",
    "\n",
    "# Randomly sample an equal number of rows from each subset\n",
    "sampled_true = true_df.sample(n=20, replace=False, random_state=42)\n",
    "sampled_fake = fake_df.sample(n=20, replace=False, random_state=42)\n",
    "\n",
    "# Combine the sampled rows into a single DataFrame\n",
    "sampled_df = pd.concat([sampled_true, sampled_fake])\n",
    "\n",
    "# Reset the index of the sampled DataFrame\n",
    "sampled_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Title</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>washington reuters united states cannot trust ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new york reuters email donald trump’s campaign...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stockholm reuters swedish court monday sentenc...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>washington reuters us president barack obama s...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bogotacartagena colombia reuters pope francis ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content         Title  Type\n",
       "0  washington reuters united states cannot trust ...  politicsNews  true\n",
       "1  new york reuters email donald trump’s campaign...  politicsNews  true\n",
       "2  stockholm reuters swedish court monday sentenc...     worldnews  true\n",
       "3  washington reuters us president barack obama s...  politicsNews  true\n",
       "4  bogotacartagena colombia reuters pope francis ...     worldnews  true"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Text Categorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextCategorizer.__init__() missing 1 required positional argument: 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextCategorizer\n\u001b[1;32m----> 2\u001b[0m textcat \u001b[38;5;241m=\u001b[39m \u001b[43mTextCategorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: TextCategorizer.__init__() missing 1 required positional argument: 'model'"
     ]
    }
   ],
   "source": [
    "from spacy.pipeline import TextCategorizer\n",
    "textcat = TextCategorizer(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textcat.add_label(\"fake\")\n",
    "textcat.add_label(\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textcat.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This is a positive review.\")\n",
    "textcat(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.pipeline.textcat import Config\n",
    "import pandas as pd\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E867] The 'textcat' component requires at least two labels because it uses mutually exclusive classes where exactly one label is True for each doc. For binary classification tasks, you can use two labels with 'textcat' (LABEL / NOT_LABEL) or alternatively, you can use the 'textcat_multilabel' component with one label.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m other_pipes \u001b[38;5;241m=\u001b[39m [pipe \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mpipe_names \u001b[38;5;28;01mif\u001b[39;00m pipe \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtextcat\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mdisable_pipes(\u001b[38;5;241m*\u001b[39mother_pipes):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# Adjust the number of epochs as needed\u001b[39;00m\n\u001b[0;32m     25\u001b[0m         losses \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\language.py:1287\u001b[0m, in \u001b[0;36mLanguage.begin_training\u001b[1;34m(self, get_examples, sgd)\u001b[0m\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbegin_training\u001b[39m(\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1282\u001b[0m     get_examples: Optional[Callable[[], Iterable[Example]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   1284\u001b[0m     sgd: Optional[Optimizer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1285\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optimizer:\n\u001b[0;32m   1286\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(Warnings\u001b[38;5;241m.\u001b[39mW089, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m-> 1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msgd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msgd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\language.py:1349\u001b[0m, in \u001b[0;36mLanguage.initialize\u001b[1;34m(self, get_examples, sgd)\u001b[0m\n\u001b[0;32m   1345\u001b[0m         p_settings \u001b[38;5;241m=\u001b[39m I[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomponents\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(name, {})\n\u001b[0;32m   1346\u001b[0m         p_settings \u001b[38;5;241m=\u001b[39m validate_init_settings(\n\u001b[0;32m   1347\u001b[0m             proc\u001b[38;5;241m.\u001b[39minitialize, p_settings, section\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomponents\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39mname\n\u001b[0;32m   1348\u001b[0m         )\n\u001b[1;32m-> 1349\u001b[0m         proc\u001b[38;5;241m.\u001b[39minitialize(get_examples, nlp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mp_settings)\n\u001b[0;32m   1350\u001b[0m pretrain_cfg \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrain_cfg:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\pipeline\\textcat.py:389\u001b[0m, in \u001b[0;36mTextCategorizer.initialize\u001b[1;34m(self, get_examples, nlp, labels, positive_label)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_label(label)\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 389\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE867)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m positive_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m positive_label \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels:\n",
      "\u001b[1;31mValueError\u001b[0m: [E867] The 'textcat' component requires at least two labels because it uses mutually exclusive classes where exactly one label is True for each doc. For binary classification tasks, you can use two labels with 'textcat' (LABEL / NOT_LABEL) or alternatively, you can use the 'textcat_multilabel' component with one label."
     ]
    }
   ],
   "source": [
    "# Prepare training data\n",
    "train_texts = df['Content'].tolist()\n",
    "train_labels = [{'cats': {'Real News': label == 'Real News', 'Fake News': label == 'Fake News'}} for label in df['Type']]\n",
    "\n",
    "# Initialize the text categorizer with default config\n",
    "textcat = nlp.create_pipe(\"textcat\", config=Config())\n",
    "\n",
    "# Add labels to the text categorizer (binary labels: Real News and Fake News)\n",
    "textcat.add_label(\"Real News\")\n",
    "textcat.add_label(\"Fake News\")\n",
    "\n",
    "# Add the text categorizer to the pipeline using the string name 'textcat'\n",
    "nlp.add_pipe('textcat')\n",
    "\n",
    "# Disable other pipelines such as tagger, parser, and ner\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    # Training the model\n",
    "    optimizer = nlp.begin_training()\n",
    "    for i in range(10):  # Adjust the number of epochs as needed\n",
    "        losses = {}\n",
    "        for texts, annotations in zip(train_texts, train_labels):\n",
    "            nlp.update([texts], [annotations], sgd=optimizer, losses=losses)\n",
    "        print(losses)\n",
    "\n",
    "# Test the trained model\n",
    "doc = nlp(\"The economy is growing at a record pace.\")\n",
    "print(doc.cats)  # This will give you the probability scores for each label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.0, 'textcat': 0.25}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.0, 'textcat': 0.252169668674469}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.0, 'textcat': 0.2500309348106384}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.0, 'textcat': 0.2541232705116272}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.0, 'textcat': 0.26121795177459717}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.0, 'textcat': 0.23484300076961517}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.0, 'textcat': 0.21698680520057678}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.0, 'textcat': 0.23045983910560608}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.0, 'textcat': 0.19577749073505402}\n",
      "{'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.0, 'textcat': 0.2603570818901062}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch\n",
    "import random\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Prepare your training data (example)\n",
    "train_data = [\n",
    "    (\"This is a positive example\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
    "    (\"This is a negative example\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}})\n",
    "]\n",
    "\n",
    "# Initialize the text categorizer component\n",
    "textcat = nlp.add_pipe(\"textcat\")\n",
    "textcat.cfg[\"exclusive_classes\"] = True\n",
    "textcat.cfg[\"architecture\"] = \"simple_cnn\"\n",
    "\n",
    "# Add the binary labels to the text categorizer\n",
    "textcat.add_label(\"POSITIVE\")\n",
    "textcat.add_label(\"NEGATIVE\")\n",
    "\n",
    "# Train the text categorizer\n",
    "random.seed(1)\n",
    "optimizer = nlp.begin_training()\n",
    "for epoch in range(10):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "    batches = minibatch(train_data, size=8)\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        example = []\n",
    "        for i in range(len(texts)):\n",
    "            doc = nlp.make_doc(texts[i])\n",
    "            example.append(Example.from_dict(doc, annotations[i]))\n",
    "        nlp.update(example, losses=losses, drop=0.2, sgd=optimizer)\n",
    "    print(losses)\n",
    "\n",
    "# Evaluate the model (optional)\n",
    "# Evaluate your model on a separate validation set to assess its performance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
