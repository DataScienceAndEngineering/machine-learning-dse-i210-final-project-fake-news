{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f768f51ff419d",
   "metadata": {},
   "source": [
    "**Abhina Premachandran Bindu**\n",
    "\n",
    "# Comparing the performance of gensim vs nltk libraries\n",
    "<p> In this analysis, the nltk and gensim nlp libraries are compared based on the accuracy scores of the same classifier applied on the processed texts corresponding to the libraries. It is found that using nltk library to process the text and tfidf vectorizer to apply the classifier resulted in better accuracy scores compared to using gensim's word2vect function for training the classifier. The accuracy for nltk is 0.99 for the gradient boosting classifier while the gensim accuracy for the same classifier is only 0.96.</p>\n",
    "\n",
    "## Loading and initial cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T11:22:57.693257Z",
     "start_time": "2024-05-11T11:22:57.576451Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing the necessary libraries\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# importing the libraries for nltk\n",
    "import nltk\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# BoW\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b005e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 --> Fake , df2 --> Real\n",
    "df1 = pd.read_csv(\"/Users/abhinapremachandran/Desktop/Spring '24 CCNY/Machine Learning/group_project_ML/Fake.csv\")\n",
    "df2 = pd.read_csv(\"/Users/abhinapremachandran/Desktop/Spring '24 CCNY/Machine Learning/group_project_ML/True.csv\")\n",
    "# adding the labels Real --> 1 and Fake --> 0\n",
    "df1['target'] = 0\n",
    "df2['target'] = 1\n",
    "# combining the dataframes\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "# shuffling the indices\n",
    "shuffled_indices = np.random.permutation(combined_df.index)\n",
    "\n",
    "# Using .loc[] to rearrange the DataFrame rows according to the shuffled indices\n",
    "data = combined_df.loc[shuffled_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a4b748",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe8e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c5d09acbbb714",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T16:58:13.210467Z",
     "start_time": "2024-04-30T16:58:13.198866Z"
    }
   },
   "outputs": [],
   "source": [
    "# dropping the na values\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380d38d8d7566387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T16:58:14.140195Z",
     "start_time": "2024-04-30T16:58:14.133683Z"
    }
   },
   "outputs": [],
   "source": [
    "# checking the value counts of 'target' to check for data imbalance\n",
    "data.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc56e418debe90",
   "metadata": {},
   "source": [
    " Since the number of Fake and True classes are almost same, there is no class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766bca5964e15350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T16:58:15.020468Z",
     "start_time": "2024-04-30T16:58:15.014725Z"
    }
   },
   "outputs": [],
   "source": [
    "data.subject.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b0b5df254691f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a981875",
   "metadata": {},
   "source": [
    "## using nltk for cleaning and preparing for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67078c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and removing stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # remove non-alphabetical characters and stopwords\n",
    "    cleaned_tokens = [re.sub(r'[^a-zA-Z ]', '', text).lower() for text in tokens if text.lower() not in stop_words]\n",
    "    cleaned_tokens = [token for token in cleaned_tokens if ((token not in  set(string.punctuation)))]\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\n",
    "    # Join the tokens back into a string\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    #stem the tokens\n",
    "    porter = PorterStemmer()\n",
    "    cleaned_text = \" \".join(porter.stem(token) for token in processed_text.split())\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the function across the DataFrame\n",
    "data_nltk = data.copy()\n",
    "data_nltk['cleaned_text'] = data_nltk['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b734a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nltk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8589b011",
   "metadata": {},
   "source": [
    "## using gensim to clean and build the vectors for the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b35f4ab0e8b04c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T16:58:28.076405Z",
     "start_time": "2024-04-30T16:58:17.473031Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the function across the DataFrame\n",
    "data_gensim = data.copy()\n",
    "data_gensim['cleaned_text'] = data_gensim['text'].apply(gensim.utils.simple_preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f613c87041eb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T16:58:28.095236Z",
     "start_time": "2024-04-30T16:58:28.079Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1529ac2",
   "metadata": {},
   "source": [
    "## Building, training and using the gensim word2vect model for getting the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b672d749b5e307",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T16:59:56.831807Z",
     "start_time": "2024-04-30T16:59:53.366397Z"
    }
   },
   "outputs": [],
   "source": [
    "# building the word2vec model\n",
    "model = gensim.models.Word2Vec(\n",
    "    window = 6,\n",
    "    min_count = 1,\n",
    "    workers = 4\n",
    ")\n",
    "model.build_vocab(data_gensim['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77f44e4c18b3166",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T17:00:30.713369Z",
     "start_time": "2024-04-30T16:59:58.692681Z"
    }
   },
   "outputs": [],
   "source": [
    "# training the model\n",
    "model.train(data_gensim['cleaned_text'], total_examples=model.corpus_count, epochs=5)\n",
    "\n",
    "# saving the model\n",
    "model.save(\"word2vec/word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f46ff7246eda5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T17:00:32.652147Z",
     "start_time": "2024-04-30T17:00:32.648113Z"
    }
   },
   "outputs": [],
   "source": [
    "model.wv.index_to_key[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd66cead82b8c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T17:00:34.545777Z",
     "start_time": "2024-04-30T17:00:34.541757Z"
    }
   },
   "outputs": [],
   "source": [
    "len(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbebf91701154dc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T17:00:52.799753Z",
     "start_time": "2024-04-30T17:00:36.271109Z"
    }
   },
   "outputs": [],
   "source": [
    "# a function for finding the average of the word vectors \n",
    "def get_average_word2vec_vector(text, model, word_dim):\n",
    "  vec = np.zeros((word_dim,))  \n",
    "  count = 0\n",
    "  for word in text:\n",
    "    if word in model.wv:  \n",
    "      vec += model.wv[word]\n",
    "      count += 1\n",
    "  if count != 0:\n",
    "    vec /= count  \n",
    "  return vec\n",
    "\n",
    "# Get word dimensions from the model\n",
    "word_dim = model.vector_size\n",
    "\n",
    "# Apply the function to each cleaned_text\n",
    "word_vectors = [get_average_word2vec_vector(text, model, word_dim) for text in data_gensim['cleaned_text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e3c485bc59c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T17:00:52.810935Z",
     "start_time": "2024-04-30T17:00:52.802352Z"
    }
   },
   "outputs": [],
   "source": [
    "# adding the word vectors to the data\n",
    "data['word_vectors'] = word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08cc3fca3edfe39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T17:00:58.306107Z",
     "start_time": "2024-04-30T17:00:58.294614Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d6fd4c",
   "metadata": {},
   "source": [
    "## Classifying the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee36f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the model\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "# importing necessary libraries for model building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "# defining the model\n",
    "clf = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492400931c593c5",
   "metadata": {},
   "source": [
    "### using nltk and tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0127fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the X and y arrays for training and testing\n",
    "X1 = data_nltk['cleaned_text'].values\n",
    "y1 = data_nltk['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31470a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.shape,y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea8bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data to train-test split\n",
    "X_train1,X_test1,y_train1,y_test1 = train_test_split(X1,y1,test_size=0.33,random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b19a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the tfidf vectorizer\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b15a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the pipeline to fit the training data\n",
    "gb_tfidf = Pipeline([\n",
    "    ('vect',tfidf),\n",
    "    ('gb clf',clf)\n",
    "])\n",
    "gb_tfidf.fit(X_train1,y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42371e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = gb_tfidf.predict(X_test1)\n",
    "# printing the classification report for validation of the model\n",
    "print(classification_report(y_test1, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac2b0cf",
   "metadata": {},
   "source": [
    "### using gensim - word2vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d63d09a35b8e1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T17:01:04.711155Z",
     "start_time": "2024-04-30T17:01:04.708716Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining X and y arrays for training and testing\n",
    "X2 = word_vectors\n",
    "y2 = data['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556f22920bc2336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T17:01:05.135937Z",
     "start_time": "2024-04-30T17:01:05.121791Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.33, random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587172b0dedee0a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T17:01:07.358359Z",
     "start_time": "2024-04-30T17:01:07.300929Z"
    }
   },
   "outputs": [],
   "source": [
    "# reshaping the input values for classifying\n",
    "X_train_2d = np.stack(X_train2)\n",
    "X_test_2d =  np.stack(X_test2)\n",
    "X_train_2d.shape , X_test_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bad347a4fe25e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T17:03:19.572238Z",
     "start_time": "2024-04-30T17:01:07.651719Z"
    }
   },
   "outputs": [],
   "source": [
    "# fitting the train data\n",
    "clf.fit(X_train_2d, y_train2)\n",
    "# predicting the test values\n",
    "y_pred2 = clf.predict(X_test_2d)\n",
    "# printing the classification report for validation of the model\n",
    "print(classification_report(y_test2, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3b72cc",
   "metadata": {},
   "source": [
    "By comparing the performance of the nlp libraries, nltk and gensim, on the GradientBoostingClassifier, the accuracy for the model that used nltk features for word processing and Tfidfvectorizer have a perfect accuracy score of 100% while the model that used gensim processing features and its word2vec word embedding have only 96% accuracy. Therefore, in conclusion, Tfidfvectorizer word embedding is much better than gensim's word2vect word embedding in helping the classifier identify classes accurately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b02b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
