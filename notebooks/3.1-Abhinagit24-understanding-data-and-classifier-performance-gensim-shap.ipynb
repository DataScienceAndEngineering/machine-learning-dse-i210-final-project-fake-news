{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f768f51ff419d",
   "metadata": {},
   "source": [
    "**Abhina Premachandran Bindu**\n",
    "**April 28 2024**\n",
    "# Using shap to understand the Classification criteria\n",
    "## Loading and initial cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim numpy scikit-learn shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T15:04:02.185292Z",
     "start_time": "2024-04-27T15:03:57.684719Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "shap.initjs()\n",
    "import transformers\n",
    "import keras\n",
    "from wordcloud import WordCloud\n",
    "from tensorflow.python.keras.engine import data_adapter\n",
    "from tensorflow.python.keras.engine.keras_tensor import KerasTensor\n",
    "# import gensim.downloader as api\n",
    "# wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a410d05af240b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:00:06.456331Z",
     "start_time": "2024-04-22T17:00:03.169731Z"
    }
   },
   "outputs": [],
   "source": [
    "#importing the data\n",
    "data = pd.read_csv(input('Enter the file path for the csv file:'))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f953265c8e649",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:00:07.399726Z",
     "start_time": "2024-04-22T17:00:07.392684Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa469c0361198",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:00:07.977642Z",
     "start_time": "2024-04-22T17:00:07.962025Z"
    }
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c5d09acbbb714",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:00:08.387678Z",
     "start_time": "2024-04-22T17:00:08.376109Z"
    }
   },
   "outputs": [],
   "source": [
    "# dropping the na values\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770d0576792227a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:00:09.187986Z",
     "start_time": "2024-04-22T17:00:09.178909Z"
    }
   },
   "outputs": [],
   "source": [
    "# dropping the redundant 'Unnamed: 0' column\n",
    "data.drop(columns='Unnamed: 0',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380d38d8d7566387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:00:09.374938Z",
     "start_time": "2024-04-22T17:00:09.368576Z"
    }
   },
   "outputs": [],
   "source": [
    "# checking the value counts of 'target' to check for data imbalance\n",
    "data.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc56e418debe90",
   "metadata": {},
   "source": [
    " Since the number of Fake and True classes are almost same, there is no class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766bca5964e15350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:00:10.486439Z",
     "start_time": "2024-04-22T17:00:10.477491Z"
    }
   },
   "outputs": [],
   "source": [
    "data.subject.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b0b5df254691f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22994fa5cd64e4e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:00:13.951223Z",
     "start_time": "2024-04-22T17:00:13.921085Z"
    }
   },
   "outputs": [],
   "source": [
    "# encoding the class labels to numerical - Real:1 and Fake:0\n",
    "class_mapping = {label: idx for idx, label in enumerate(np.unique(data['target']))}\n",
    "data['target'] = data['target'].map(class_mapping)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b35f4ab0e8b04c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:00:28.685653Z",
     "start_time": "2024-04-22T17:00:16.612222Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the function across the DataFrame\n",
    "data['cleaned_text'] = data['text'].apply(gensim.utils.simple_preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f613c87041eb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:00:28.698937Z",
     "start_time": "2024-04-22T17:00:28.689335Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f629a1e0649970",
   "metadata": {},
   "source": [
    "## Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f11dd8df7a632",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:00:12.205155Z",
     "start_time": "2024-04-22T17:00:11.999022Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining the real dataset and fake dataset\n",
    "data_real = data[data['target']==1]\n",
    "data_fake = data[data['target']==0]\n",
    "# visualize the distribution of subjects in both real and fake data\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "# Plot the subjects in real news on the first subplot\n",
    "axs[0].hist(data_real['subject'],bins=len(data_real.subject.unique()), align = 'mid', edgecolor='black')\n",
    "axs[0].set_xlabel('subjects')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "axs[0].set_title('Subject distribution in Real News')\n",
    "axs[0].legend(['Real News'])\n",
    "\n",
    "# Plot the subjects in fake news on the second subplot\n",
    "axs[1].hist(data_fake['subject'],bins=len(data_fake.subject.unique()), align = 'mid', edgecolor='black', color = 'red')\n",
    "axs[1].set_xlabel('subjects')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "axs[1].set_title('Subject distribution in Fake News')\n",
    "axs[1].legend(['Fake News'])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967f1c2e",
   "metadata": {},
   "source": [
    " From the histogram of subjects above, it is clear that there are only two subject areas where most of the real news is focused on - 'politicsNews' and 'worldnews'. Whereas, the fake news spans to a wide variety of subject areas - 'News','politics','left-news','Government News', 'US_News','Middle-east'. Most of them are not regular news subject areas- indicating the fakeness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b5fce8963c290d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:44:17.487998Z",
     "start_time": "2024-04-22T17:42:22.968175Z"
    }
   },
   "outputs": [],
   "source": [
    "# using the Counter function to get the count of words to find the most frequent words \n",
    "from collections import Counter\n",
    "all_real_words = []\n",
    "all_fake_words = []\n",
    "for i,text in enumerate(data['cleaned_text']):\n",
    "    for word in text:\n",
    "        if data.iloc[i,2] == 1:\n",
    "            all_real_words.append(word)\n",
    "        else:\n",
    "            all_fake_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19e23ca386b6d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_real = Counter(all_real_words)\n",
    "word_counts_fake = Counter(all_fake_words)\n",
    "\n",
    "most_common_words_real = word_counts_real.most_common(20)\n",
    "most_common_words_fake = word_counts_fake.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b558cc0e0974b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining separate new dataframes for most common words in the rela and fake text classes\n",
    "common_words_realdf = pd.DataFrame(most_common_words_real, columns=['Word', 'Frequency'])\n",
    "common_words_fakedf = pd.DataFrame(most_common_words_fake, columns=['Word', 'Frequency'])\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "# Plot the most frequent words in real news on the first subplot\n",
    "axs[0].bar(common_words_realdf['Word'], common_words_realdf['Frequency'], color = 'blue')\n",
    "axs[0].set_xlabel('Word')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "axs[0].set_title('Most Frequent Words in Real News')\n",
    "axs[0].tick_params(axis='x', rotation=45)  # Rotate x-axis labels for readability\n",
    "axs[0].legend(['Real News'])\n",
    "\n",
    "# Plot the most frequent words in fake news on the second subplot\n",
    "axs[1].bar(common_words_fakedf['Word'], common_words_fakedf['Frequency'], color = 'red')\n",
    "axs[1].set_xlabel('Word')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "axs[1].set_title('Most Frequent Words in Fake News')\n",
    "axs[1].tick_params(axis='x', rotation=45)  \n",
    "axs[1].legend(['Fake News'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a2e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the words in \n",
    "text_real = ' '.join(data_real.text)\n",
    "text_fake = ' '.join(data_fake.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec533b9d63f4133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and generate a word cloud image:\n",
    "wordcloud1 = WordCloud().generate(text_real)\n",
    "wordcloud2 = WordCloud().generate(text_fake)\n",
    "\n",
    "# Display the generated image:\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 12))\n",
    "axs[0].imshow(wordcloud1, interpolation='bilinear')\n",
    "axs[0].set_title('Wordcloud of real news data')\n",
    "axs[1].imshow(wordcloud2, interpolation='bilinear')\n",
    "axs[1].set_title('Wordcloud of fake news data')\n",
    "# plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8397213",
   "metadata": {},
   "source": [
    "<p>Both the barcharts and the wordclouds above indicates that the most frequent words in both real and fake news comprise of the same words. Therefore, one can't conclude the news to be fake or real based on the distribution of words alone. </p>The only words that are different in both classes is 'Government' and 'reuters'. 'reuters' indicate the name of a credible news source, while 'Government' indicate the authority. Therefore, the fake news fails to indicate the credibility of the news they convey by citing the authority or having a credible identity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1529ac2",
   "metadata": {},
   "source": [
    "## Building, training and using the gensim word2vect model for getting the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b672d749b5e307",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:00:33.059974Z",
     "start_time": "2024-04-22T17:00:28.700007Z"
    }
   },
   "outputs": [],
   "source": [
    "# building the word2vec model\n",
    "model = gensim.models.Word2Vec(\n",
    "    window = 6,\n",
    "    min_count = 1,\n",
    "    workers = 4\n",
    ")\n",
    "model.build_vocab(data['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77f44e4c18b3166",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:01:15.079286Z",
     "start_time": "2024-04-22T17:00:33.063032Z"
    }
   },
   "outputs": [],
   "source": [
    "# training the model\n",
    "model.train(data['cleaned_text'], total_examples=model.corpus_count, epochs=5)\n",
    "\n",
    "# saving the model\n",
    "model.save(\"word2vec/word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f46ff7246eda5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:01:15.087410Z",
     "start_time": "2024-04-22T17:01:15.081384Z"
    }
   },
   "outputs": [],
   "source": [
    "model.wv.index_to_key[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd66cead82b8c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:01:15.092305Z",
     "start_time": "2024-04-22T17:01:15.088668Z"
    }
   },
   "outputs": [],
   "source": [
    "len(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbebf91701154dc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:01:31.699238Z",
     "start_time": "2024-04-22T17:01:15.093982Z"
    }
   },
   "outputs": [],
   "source": [
    "# a function for finding the average of the word vectors \n",
    "def get_average_word2vec_vector(text, model, word_dim):\n",
    "  vec = np.zeros((word_dim,))  \n",
    "  count = 0\n",
    "  for word in text:\n",
    "    if word in model.wv:  \n",
    "      vec += model.wv[word]\n",
    "      count += 1\n",
    "  if count != 0:\n",
    "    vec /= count  \n",
    "  return vec\n",
    "\n",
    "# Get word dimensions from the model\n",
    "word_dim = model.vector_size\n",
    "\n",
    "# Apply the function to each cleaned_text\n",
    "word_vectors = [get_average_word2vec_vector(text, model, word_dim) for text in data['cleaned_text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e3c485bc59c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:01:31.709888Z",
     "start_time": "2024-04-22T17:01:31.701562Z"
    }
   },
   "outputs": [],
   "source": [
    "# adding the word vectors to the data\n",
    "data['word_vectors'] = word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08cc3fca3edfe39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:02:15.811086Z",
     "start_time": "2024-04-22T17:02:15.795481Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492400931c593c5",
   "metadata": {},
   "source": [
    "## Classifying the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78df15071a62378",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:04:30.860678Z",
     "start_time": "2024-04-22T17:04:30.020940Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing necessary libraries for model building\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d63d09a35b8e1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:04:33.656867Z",
     "start_time": "2024-04-22T17:04:33.653901Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining X and y \n",
    "X = word_vectors\n",
    "y = data['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556f22920bc2336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:06:58.215927Z",
     "start_time": "2024-04-22T17:06:58.190342Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587172b0dedee0a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:07:00.855224Z",
     "start_time": "2024-04-22T17:07:00.778698Z"
    }
   },
   "outputs": [],
   "source": [
    "# reshaping the input values for classifying\n",
    "X_train_2d = np.stack(X_train)\n",
    "X_test_2d =  np.stack(X_test)\n",
    "X_train_2d.shape , X_test_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bad347a4fe25e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:09:33.240494Z",
     "start_time": "2024-04-22T17:07:19.410702Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing the model\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "# defining the model, fitting and predicting value for X_test.\n",
    "clf = GradientBoostingClassifier()\n",
    "#clf = RandomForestClassifier()\n",
    "\n",
    "clf.fit(X_train_2d, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_2d)\n",
    "# printing the classification report for validation of the model\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb98bc",
   "metadata": {},
   "source": [
    " The Graident Boosting classifier classifies the text data as real or fake with an accuracy score of 96%.\n",
    " ## Understanding the performance of the classifier - using shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1f4377",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(clf, X_train_2d)\n",
    "shap_values = explainer(X_test_2d)\n",
    "shap.plots.waterfall(shap_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4549a",
   "metadata": {},
   "source": [
    " The shap plot(waterfall) of the shap values indicate that Features 94,93, and 82 are the ones that mostly decided whether a text is real or fake. Further analysis need to be done to find the words corresponding to the features used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485f3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
