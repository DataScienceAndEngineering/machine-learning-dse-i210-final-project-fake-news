{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f66a53480a5ffc",
   "metadata": {},
   "source": [
    "# Model Testing and Selection\n",
    "   <p>I am focusing on finding the best model that classifies the data to real and fake. For this, the ML classification algorithms: LogisticRegression,QuadraticDiscriminantAnalysis, SVC, LinearSVC, DecisionTreeClassifier, XGBoost, AdaBoost, Perceptron, KNN, GaussianNB, BaggingClassifier are being used. For faster computation, the dataset is sampled down to a size of 2000.</p>\n",
    "    <p>Further Gridsearch analysis is required to find the best parameters that results in the best models that can be fitted for training.</p>\n",
    "    \n",
    "#### Initial loading and cleaning of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3831ce87c3ecee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data from the downloaded csv file\n",
    "file_path = input(\"Enter the path of the data file\")\n",
    "data = pd.read_csv(file_path)\n",
    "data.drop(columns='Unnamed: 0',inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# encoding the class labels to numerical\n",
    "class_mapping = {label: idx for idx, label in enumerate(np.unique(data['target']))}\n",
    "data['target'] = data['target'].map(class_mapping)\n",
    "\n",
    "# subsetting the data frame for faster computation\n",
    "sample_size = 2000\n",
    "data_sample = data.sample(n=sample_size,random_state=22)\n",
    "\n",
    "# Tokenize and removing stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # remove non-alphabetical characters and stopwords\n",
    "    cleaned_tokens = [re.sub(r'[^a-zA-Z ]', '', text).lower() for text in tokens if text.lower() not in stop_words]\n",
    "    cleaned_tokens = [token for token in cleaned_tokens if ((token not in  set(string.punctuation)))]\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\n",
    "    # Join the tokens back into a string\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    #stem the tokens\n",
    "    porter = PorterStemmer()\n",
    "    cleaned_text = \" \".join(porter.stem(token) for token in processed_text.split())\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the function across the DataFrame\n",
    "data_sample['cleaned_text'] = data_sample['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d336e28741eeda94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the X and y \n",
    "X = data_sample['cleaned_text'].values\n",
    "y = data_sample['target'].values\n",
    "# splitting data to train-test split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=22)\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8920c1cefeddda",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c663c82f8cbbe5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the classification algorithms\n",
    "\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a7dafedf3300bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling all the ML classification algorithms imported above\n",
    "ppn = Perceptron(eta0=0.1, random_state=1)\n",
    "lr_solver1 = LogisticRegression(C=100.0, solver='lbfgs', multi_class='ovr')\n",
    "lr_solver2 = LogisticRegression(C=100.0, solver='liblinear', multi_class='ovr')\n",
    "nb = GaussianNB()\n",
    "knn = KNeighborsClassifier(n_neighbors=5, p=2)\n",
    "QDA = QuadraticDiscriminantAnalysis()\n",
    "svm_linear = SVC(kernel='linear', C=1.0, random_state=1)\n",
    "svm_rbf = SVC(kernel='rbf', random_state=1, gamma=0.10, C=10.0)\n",
    "linear_svc = LinearSVC(dual=\"auto\", random_state=0, tol=1e-5)\n",
    "tree_gini = DecisionTreeClassifier(criterion='gini',max_depth=4,random_state=1)\n",
    "tree_entropy = DecisionTreeClassifier(criterion='entropy',max_depth=4,random_state=1)\n",
    "abc = AdaBoostClassifier(algorithm='SAMME',n_estimators=100,learning_rate=0.1,random_state=1)\n",
    "RF = RandomForestClassifier(n_estimators=20,random_state=1,n_jobs=2)\n",
    "bag = BaggingClassifier(n_estimators=100,max_samples=1.0,max_features=1.0,bootstrap=True,bootstrap_features=False,n_jobs=1,random_state=1)\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.01,max_depth=4, random_state=1,use_label_encoder=False)\n",
    "mlp = MLPClassifier(alpha=1, max_iter=100, random_state=42)\n",
    "GPC = GaussianProcessClassifier(1.0 * RBF(1.0), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08e9ef31737866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a dictionary containing all the algorithms and their names\n",
    "clf_dict = {'perceptron':ppn, 'Log Reg lbfgs':lr_solver1, 'Log Reg liblinear':lr_solver2, 'KNN':knn, 'Linear kernel svm':svm_linear, 'RBF kernel svm':svm_rbf, 'Linear SVC':linear_svc, 'Decision Tree gini':tree_gini,'Decision Tree entropy':tree_entropy, 'AdaBoost':abc, 'RandomForest':RF,'Bagging Clf':bag,'xgb':xgb_model}\n",
    "\n",
    "for clf_name, clf in clf_dict.items():\n",
    "    clf_tfidf = Pipeline([\n",
    "    ('vect',tfidf),\n",
    "    (clf_name,clf)])\n",
    "    clf_tfidf.fit(X_train,y_train)\n",
    "    print(f'Test Accuracy for {clf_name}: {clf_tfidf.score(X_test, y_test):.3f}')\n",
    "    print('----------------------------------------------')\n",
    "    \n",
    "#'GaussianNB':nb,\n",
    "#'Quadratic Discriminant Analysis':QDA,\n",
    "#,'MLP':mlp,'Gaussian PC':GPC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b03af9a7c8b813",
   "metadata": {},
   "source": [
    " From the preliminary analysis, out of the algorithms tested for accuracy, it is found that Decision tree, Adaboost, Bagging, and xgb have the highest accuracy score of 0.992 for the sampled dataset. Knn performed the worst with a score of 0.763."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e48a558a6da43d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
